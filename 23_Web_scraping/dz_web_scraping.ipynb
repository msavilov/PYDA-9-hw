{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1.\n",
    "\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['python', 'парсинг']\n",
    "URL = 'https://habr.com/ru/all/'\n",
    "req = requests.get(URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "posts = soup.find_all('article', class_='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Так как вариант с preview-информацией разбирался на занятии, сразу делал необязательное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://habr.com/ru/post/517056/', 'https://habr.com/ru/post/517044/']\n"
     ]
    }
   ],
   "source": [
    "all_links = []\n",
    "for post in posts:\n",
    "    post_id = post.parent.attrs.get('id')\n",
    "    if not post_id:\n",
    "        continue\n",
    "    post_id = int(post_id.split('_')[-1])\n",
    "    title_element = post.find('a', class_='post__title_link')\n",
    "    link = title_element.attrs.get('href') # получаем ссылку для каждого поста\n",
    "    soup_temp = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    text_link = soup_temp.find('div', class_='post__text').text #получаем содержимое поста по ссылке\n",
    "    for search_word in KEYWORDS: \n",
    "        if search_word in text_link: #проверяем наличие слова в тексте поста\n",
    "            all_links.append(link)\n",
    "            break\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>Интерактивная визуализация алгоритмов на базе ...</td>\n",
       "      <td>https://habr.com/ru/post/517056/</td>\n",
       "      <td>Jupyter уже давно зарекомендовал себя как удоб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>Бот в телеграм, озвучивающий ваши эмоции в соо...</td>\n",
       "      <td>https://habr.com/ru/post/517044/</td>\n",
       "      <td>Привет!\\nВ этой статье я опишу своего бота в т...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2020-08-29  Интерактивная визуализация алгоритмов на базе ...   \n",
       "0  2020-08-29  Бот в телеграм, озвучивающий ваши эмоции в соо...   \n",
       "\n",
       "                               link  \\\n",
       "0  https://habr.com/ru/post/517056/   \n",
       "0  https://habr.com/ru/post/517044/   \n",
       "\n",
       "                                                text  \n",
       "0  Jupyter уже давно зарекомендовал себя как удоб...  \n",
       "0  Привет!\\nВ этой статье я опишу своего бота в т...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habr_news = pd.DataFrame()\n",
    "for link in all_links:\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    date = pd.to_datetime(soup.find('span', class_='post__time').get('data-time_published'), dayfirst=True).date()\n",
    "    title = soup.find('span', class_='post__title-text').text\n",
    "    text = soup.find('div', class_='post__text').text\n",
    "    row = {'date': date, 'title': title, 'link': link, 'text': text}\n",
    "    habr_news = pd.concat([habr_news, pd.DataFrame([row])]) \n",
    "habr_news   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2.\n",
    "Обязательная часть\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\":\"ok\",\"value\":[{\"leak_id\":\"1631f5e3-9b31-4a78-be91-111a01a87b56\",\"username\":\"cabeh0k\",\"domain\":\"\",\"passwords\":[{\"password\":null,\"encrypted\":false}],\"leak_info\":{\"id\":\"1631f5e3-9b31-4a78-be91-111a01a87b56\",\"date\":1576195200000,\"title\":\"Sensitive Source\",\"description\":\"This source has been marked as sensitive due to one of the following reasons: Revealing the source may compromise an on-going investigation. The affected site is of a controversial nature but does not validate email addresses and could therefore be used to tarnish an employee\\'s reputation.\",\"number_of_entries\":19009637,\"source_references\":[],\"media_references\":[],\"domains\":[\"\"],\"picture_url\":null,\"service_name\":null},\"marked_resolved\":false,\"leak_date\":1576195200000,\"passwords_count\":1},{\"leak_id\":\"2e6d3973-2110-4bdc-aca1-681d8b2eabfd\",\"username\":\"caba7@mail.ru\",\"domain\":\"storelp.ru\",\"passwords\":[{\"password\":null,\"encrypted\":true}],\"leak_info\":{\"id\":\"2e6d3973-2110-4bdc-aca1-681d8b2eabfd\",\"date\":1529366400000,\"title\":\"StoreLP.ru\",\"description\":\"At an unconfirmed date, StoreLP.ru\\'s database was allegedly breached. The stolen data contains passwords and email addresses. This breach is being privately shared on the internet.\",\"number_of_entries\":1435760,\"source_references\":[],\"media_references\":[],\"domains\":[\"storelp.ru\"],\"picture_url\":null,\"service_name\":null},\"marked_resolved\":false,\"leak_date\":1529366400000,\"passwords_count\":1},{\"leak_id\":\"b06f585f-ccc3-48f0-8bf8-c0ebddfe400b\",\"username\":\"caba7@mail.ru\",\"domain\":\"qip.ru\",\"passwords\":[{\"password\":null,\"encrypted\":false}],\"leak_info\":{\"id\":\"b06f585f-ccc3-48f0-8bf8-c0ebddfe400b\",\"date\":1477958400000,\"title\":\"Qip\",\"description\":\"In 2011, Russian instant messaging service provider QIP was allegedly breached. The leaked data included over 33 million user records containing login details as well as other personal information and passwords stored in plain text. \",\"number_of_entries\":29271007,\"source_references\":[],\"media_references\":[\"https://uk.news.yahoo.com/2016-mega-breaches-continue-hackers-105250221.html\"],\"domains\":[\"qip.ru\"],\"picture_url\":null,\"service_name\":null},\"marked_resolved\":false,\"leak_date\":1477958400000,\"passwords_count\":1},{\"leak_id\":\"1f0fa5ee-a1a0-4aa1-9b5c-b839521a1d2c\",\"username\":\"caba7@mail.ru\",\"domain\":\"\",\"passwords\":[{\"password\":null,\"encrypted\":true}],\"leak_info\":{\"id\":\"1f0fa5ee-a1a0-4aa1-9b5c-b839521a1d2c\",\"date\":1575504000000,\"title\":\"Sensitive Source\",\"description\":\"This source has been marked as sensitive due to one of the following reasons: Revealing the source may compromise an on-going investigation. The affected site is of a controversial nature but does not validate email addresses and could therefore be used to tarnish an employee\\'s reputation.\",\"number_of_entries\":29733746,\"source_references\":[],\"media_references\":[],\"domains\":[\"\"],\"picture_url\":null,\"service_name\":null},\"marked_resolved\":false,\"leak_date\":1575504000000,\"passwords_count\":1},{\"leak_id\":\"1b59304f-cd33-49f0-bcbb-ee5e00c73d32\",\"username\":\"caba7@mail.ru\",\"domain\":\"\",\"passwords\":[{\"password\":null,\"encrypted\":false}],\"leak_info\":{\"id\":\"1b59304f-cd33-49f0-bcbb-ee5e00c73d32\",\"date\":1597276800000,\"title\":\"\\\\\"AntiPub Drugs Squad\\\\\" Combolist\",\"description\":\"In August 2020, this combolist was shared on a private hacking forum. The proliferation of stolen or leaked databases has given rise to credential stuffing, a fairly simple technique in which criminals load lists of previously breached credentials (from Linkedin, MySpace, etc.), called combo lists, into automated brute-forcing tools to test credentials en masse. These tools test stolen passwords against thousands of targeted websites and applications until there is a match.\",\"number_of_entries\":106988173,\"source_references\":[],\"media_references\":[],\"domains\":[\"\"],\"picture_url\":null,\"service_name\":null},\"marked_resolved\":false,\"leak_date\":1597276800000,\"passwords_count\":1}]}'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://digibody.avast.com/v1/web/leaks'\n",
    "# EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "req = requests.post(URL, json={\"email\": \"caba7@mail.ru\"})\n",
    "req.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
